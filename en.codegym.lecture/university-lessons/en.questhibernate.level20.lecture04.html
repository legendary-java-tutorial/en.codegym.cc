BigData: Techniques and Strategies for Developing MapReduce Applications
<p>----------------------------------------</p>
Map only job. combine. Chains of MapReduce tasks. distributed cache. Reduce Join. mapjoin.
<p>----------------------------------------</p>
<h2>5.1 Map only job</h2>
<p>It's time to describe various techniques that allow you to effectively use MapReduce to solve practical problems, as well as show some of the features of Hadoop that can simplify development or significantly speed up the execution of a MapReduce task on a cluster.</p>
<p>As we remember, MapReduce consists of Map, Shuffle and Reduce stages. As a rule, the Shuffle stage turns out to be the most difficult in practical tasks, since data is sorted at this stage. In fact, there are a number of tasks in which the Map stage alone can be dispensed with. Here are examples of such tasks:</p>
<ul>
 <li>Data filtering (for example, "Find all records from the IP address 123.123.123.123" in the web server logs);</li>
 <li>Data transformation (“Delete column in csv-logs”);</li>
 <li>Loading and unloading data from an external source (“Insert all records from the log into the database”).</li>
</ul>
<p>Such tasks are solved using Map-Only. When creating a Map-Only task in Hadoop, you need to specify zero number of reducers:</p><img data-max-width="512" data-id="38d1e29c-fa4f-417d-ab05-44791ce19834" alt="" src="https://cdn.javarush.com/images/article/38d1e29c-fa4f-417d-ab05-44791ce19834/512.jpeg" style="width: 512px;">
<p>An example of a map-only task configuration on hadoop:</p>
<div class="table-container">
 <table>
  <tbody>
   <tr>
    <th>native interface</th>
    <th>Hadoop Streaming Interface</th>
   </tr>
   <tr>
    <td>
     <p>Specify zero number of reducers when configuring job'a:</p>
     <pre><code>job.setNumReduceTasks(0); 
</code></pre></td>
    <td>
     <p>We do not specify a reducer and specify a zero number of reducers. Example:</p>
     <pre><code>hadoop jar hadoop-streaming.jar \ 
 -D mapred.reduce.tasks=0\ 
-input input_dir\ 
-output output_dir\ 
-mapper "python mapper.py"\ 
-file "mapper.py"
</code></pre></td>
   </tr>
  </tbody>
 </table>
</div>
<p>Map Only jobs can actually be very useful. For example, in the Facetz.DCA platform, to identify the characteristics of users by their behavior, it is precisely one large map-only that is used, each mapper of which takes a user as an input and returns his characteristics as an output.</p>
<h2>5.2 Combine</h2>
<p>As I already wrote, usually the most difficult stage when performing a Map-Reduce task is the shuffle stage. This happens because the intermediate results (mapper's output) are written to disk, sorted and transmitted over the network. However, there are tasks in which such behavior does not seem very reasonable. For example, in the same task of counting words in documents, you can pre-aggregate the results of the outputs of several mappers on one map-reduce node of the task, and pass the already summed values ​​for each machine to the reducer.</p><img data-max-width="512" data-id="5d08e85a-d51f-43eb-8dad-32824da03ce9" alt="" src="https://cdn.javarush.com/images/article/5d08e85a-d51f-43eb-8dad-32824da03ce9/512.jpeg" style="width: 512px;">
<p>In hadoop, for this, you can define a combining function that will process the output of part of the mappers. The combining function is very similar to reduce - it takes the output of some mappers as input and produces an aggregated result for these mappers, so the reducer is often used as a combiner as well. An important difference from reduce is <strong>that not all values ​​corresponding to one key get to the combining function</strong> .</p>
<p>Moreover, hadoop does not guarantee that the combine function will be executed at all for the output of the mapper. Therefore, the combining function is not always applicable, for example, in the case of searching for the median value by key. Nevertheless, in those tasks where the combining function is applicable, its use allows to achieve a significant increase in the speed of the MapReduce task.</p>
<p>Using the Combiner on hadoop:</p>
<div class="table-container">
 <table>
  <tbody>
   <tr>
    <th>native interface</th>
    <th>Hadoop streaming</th>
   </tr>
   <tr>
    <td>
     <p>When configuring job-a, specify the class-Combiner. As a rule, it is the same as Reducer:</p>
     <pre><code>job.setMapperClass(TokenizerMapper.class); 
job.setCombinerClass(IntSumReducer.class); 
job.setReducerClass(IntSumReducer.class); 
</code></pre></td>
    <td>
     <p>Specify the -combiner command in the command line options. Typically, this command is the same as the reducer command. Example:</p>
     <pre><code>hadoop jar hadoop-streaming.jar \ 
-input input_dir\ 
-output output_dir\ 
-mapper "python mapper.py"\ 
-reducer "python reducer.py"\ 
-combiner "python reducer.py"\ 
-file "mapper.py"\ 
-file "reducer.py"\
</code></pre></td>
   </tr>
  </tbody>
 </table>
</div>
<h2>5.3 MapReduce task chains</h2>
<p>There are situations when one MapReduce is not enough to solve a problem. For example, consider a slightly modified WordCount task: there is a set of text documents, you need to count how many words occurred from 1 to 1000 times in the set, how many words from 1001 to 2000, how many from 2001 to 3000, and so on. For the solution, we need 2 MapReduce jobs:</p>
<ul>
 <li>Modified wordcount, which for each word will calculate which of the intervals it fell into;</li>
 <li>A MapReduce that counts how many times each interval was encountered in the output of the first MapReduce.</li>
</ul>
<p>Pseudo code solution:</p>
<div class="table-container">
 <table>
  <tbody>
   <tr>
    <td>
     <pre><code>#map1 
def map(doc): 
for word in doc: 
yield word, 1</code></pre></td>
    <td>
     <pre><code>#reduce1 
def reduce(word, values): 
yield int(sum(values)/1000), 1 </code></pre></td>
   </tr>
   <tr>
    <td>
     <pre><code>#map2 
def map(doc): 
interval, cnt = doc.split() 
yield interval, cnt </code></pre></td>
    <td>
     <pre><code>#reduce2 
def reduce(interval, values): 
yield interval*1000, sum(values) </code></pre></td>
   </tr>
  </tbody>
 </table>
</div>
<p>In order to execute a sequence of MapReduce tasks on hadoop, it is enough just to specify the folder that was specified as output for the first one as input for the second task and run them in turn.</p>
<p>In practice, chains of MapReduce tasks can be quite complex sequences in which MapReduce tasks can be connected both sequentially and in parallel to each other. To simplify the management of such task execution plans, there are separate tools like oozie and luigi, which will be discussed in a separate article in this series.</p><img data-max-width="512" data-id="e75b9c38-86ca-4edc-a989-48ec6139d6ca" alt="" src="https://cdn.javarush.com/images/article/e75b9c38-86ca-4edc-a989-48ec6139d6ca/512.jpeg" style="width: 512px;">
<h2>5.4 Distributed cache</h2>
<p>An important mechanism in Hadoop is the Distributed Cache. Distributed Cache allows you to add files (eg text files, archives, jar files) to the environment where the MapReduce task is running.</p>
<p>You can add files stored on HDFS, local files (local to the machine from which the task is launched). I've already implicitly shown how to use Distributed Cache with hadoop streaming by adding the mapper.py and reducer.py files via the -file option. In fact, you can add not only mapper.py and reducer.py, but arbitrary files in general, and then use them as if they were in a local folder.</p>
<p>Using Distributed Cache:</p>
<div class="table-container">
 <tody>
 </tody>
 <table>
  <tbody>
   <tr>
    <th>Native API</th>
   </tr>
   <tr>
    <td>
     <pre class=" line-numbers language-java" tabindex="0"><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span><code class=" language-java"><span class="token comment">//Job configuration</span>
<span class="token class-name">JobConf</span> job <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">JobConf</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">DistributedCache</span><span class="token punctuation">.</span><span class="token function">addCacheFile</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token function">URI</span><span class="token punctuation">(</span><span class="token string">"/myapp/lookup.dat#lookup.dat"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  job<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">DistributedCache</span><span class="token punctuation">.</span><span class="token function">addCacheArchive</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token function">URI</span><span class="token punctuation">(</span><span class="token string">"/myapp/map.zip"</span><span class="token punctuation">,</span> job<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">DistributedCache</span><span class="token punctuation">.</span><span class="token function">addFileToClassPath</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"/myapp/mylib.jar"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> job<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">DistributedCache</span><span class="token punctuation">.</span><span class="token function">addCacheArchive</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token function">URI</span><span class="token punctuation">(</span><span class="token string">"/myapp/mytar.tar"</span><span class="token punctuation">,</span> job<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">DistributedCache</span><span class="token punctuation">.</span><span class="token function">addCacheArchive</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token function">URI</span><span class="token punctuation">(</span><span class="token string">"/myapp/mytgz.tgz"</span><span class="token punctuation">,</span> job<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">//example of usage in mapper-e:</span>
<span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">MapClass</span> <span class="token keyword">extends</span> <span class="token class-name">MapReduceBase</span>
<span class="token keyword">implements</span> <span class="token class-name">Mapper</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">K</span><span class="token punctuation">,</span> <span class="token class-name">V</span><span class="token punctuation">,</span> <span class="token class-name">K</span><span class="token punctuation">,</span> <span class="token class-name">V</span><span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>

 <span class="token keyword">private</span> <span class="token class-name">Path</span><span class="token punctuation">[</span><span class="token punctuation">]</span> localArchives<span class="token punctuation">;</span>
 <span class="token keyword">private</span> <span class="token class-name">Path</span><span class="token punctuation">[</span><span class="token punctuation">]</span> localFiles<span class="token punctuation">;</span>

 <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">configure</span><span class="token punctuation">(</span><span class="token class-name">JobConf</span> job<span class="token punctuation">)</span> <span class="token punctuation">{</span>
   <span class="token comment">// get cached data from archives</span>
   <span class="token class-name">File</span> f <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">File</span><span class="token punctuation">(</span><span class="token string">"./map.zip/some/file/in/zip.txt"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>

 <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span><span class="token class-name">K</span> key<span class="token punctuation">,</span> <span class="token class-name">V</span> value<span class="token punctuation">,</span>
             	<span class="token class-name">OutputCollector</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">K</span><span class="token punctuation">,</span> <span class="token class-name">V</span><span class="token punctuation">&gt;</span></span> output<span class="token punctuation">,</span> <span class="token class-name">Reporter</span> reporter<span class="token punctuation">)</span>
 <span class="token keyword">throws</span> <span class="token class-name">IOException</span> <span class="token punctuation">{</span>
   <span class="token comment">// use data here</span>
   <span class="token comment">// ...</span>
   <span class="token comment">// ...</span>
   output<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> v<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span></code></pre></td>
   </tr>
  </tbody>
 </table>
</div>
<div class="table-container">
 <table>
  <tbody>
   <tr>
    <th>Hadoop Streaming</th>
   </tr>
   <tr>
    <td>
     <p>#we list the files that need to be added to the distributed cache in the –files parameter. The --files option must come before the other options.</p>
     <pre><code>yarn  hadoop-streaming.jar\ 
-files mapper.py,reducer.py,some_cached_data.txt\ 
-input '/some/input/path' \ 
-output '/some/output/path' \  
-mapper 'python mapper.py' \ 
-reducer 'python reducer.py' \
</code></pre>
     <p>usage example:</p>
     <pre><code>import sys 
#just read file from local folder 
data = open('some_cached_data.txt').read() 
 
for line in sys.stdin() 
#processing input 
#use data here
</code></pre></td>
   </tr>
  </tbody>
 </table>
</div>
<h2>5.5 Reduce Join</h2>
<p>Those who are accustomed to working with relational databases often use the very convenient Join operation, which allows them to jointly process the contents of some tables by joining them according to some key. When working with big data, this problem also sometimes arises. Consider the following example:</p>
<p>There are logs of two web servers, each log looks like this:</p>
<pre><code>t\t
</code></pre>
<p>Log snippet example:</p>
<pre><code>1446792139	
178.78.82.1	
/sphingosine/unhurrying.css 
1446792139	
126.31.163.222	
/accentually.js 
1446792139	
154.164.149.83	
/pyroacid/unkemptly.jpg 
1446792139	
202.27.13.181	
/Chawia.js 
1446792139	
67.123.248.174	
/morphographical/dismain.css 
1446792139	
226.74.123.135	
/phanerite.php 
1446792139	
157.109.106.104	
/bisonant.css
</code></pre>
<p>It is necessary to calculate for each IP address which of the 2 servers it visited more often. The result should be in the form:</p>
<pre><code>\t
</code></pre>
<p>An example of a part of the result:</p>
<pre><code>178.78.82.1	
first 
126.31.163.222	
second 
154.164.149.83	
second 
226.74.123.135	
first
</code></pre>
<p>Unfortunately, unlike relational databases, in general, joining two logs by key (in this case, by IP address) is a rather heavy operation and is solved using 3 MapReduce and the Reduce Join pattern:</p><img data-max-width="512" data-id="2398da09-c1ab-4233-a8ff-83f07aa50d55" alt="" src="https://cdn.javarush.com/images/article/2398da09-c1ab-4233-a8ff-83f07aa50d55/512.jpeg" style="width: 512px;">
<p>ReduceJoin works like this:</p>
<p><strong>1)</strong> For each of the input logs, a separate MapReduce (Map only) is launched, converting the input data to the following form:</p>
<pre><code>key -&gt; (type, value
</code></pre>
<p>Where key is the key to join tables on, Type is the type of the table (first or second in our case), and Value is any additional data bound to the key.</p>
<p><strong>2)</strong> The outputs of both MapReduces are fed to the input of the 3rd MapReduce, which, in fact, performs the union. This MapReduce contains an empty Mapper that simply copies the input. Next, shuffle decomposes the data into keys and feeds it to the reducer as input:</p>
<pre><code>key -&gt; [(type, value)]
</code></pre>
<p>It is important that at this moment the reducer receives records from both logs, and at the same time, it is possible to identify by the type field which of the two logs a particular value came from. So there is enough data to solve the original problem. In our case, the reducer simply has to calculate for each record key which type has encountered more and output this type.</p>
<h2>5.6 MapJoins</h2>
<p>The ReduceJoin pattern describes the general case of joining two logs by key. However, there is a special case in which the task can be significantly simplified and accelerated. This is the case in which one of the logs is significantly smaller than the other. Consider the following problem:</p>
<p>There are 2 logs. The first log contains the web server log (same as in the previous task), the second file (100kb in size) contains the URL-&gt; Theme match. Example 2nd file:</p>
<pre><code>/toyota.php 	
auto 
/football/spartak.html 	
sport 
/cars 	
auto 
/finances/money 	
business
</code></pre>
<p>For each IP address, it is necessary to calculate the pages of which category from this IP address were loaded most often.</p>
<p>In this case, we also need to join 2 logs by URL. However, in this case, we do not have to run 3 MapReduce, since the second log will completely fit into memory. In order to solve the problem using the 1st MapReduce, we can load the second log into the Distributed Cache, and when the Mapper is initialized, simply read it into memory, putting it in the -&gt; topic dictionary.</p>
<p>Further, the problem is solved as follows:</p>
<p><strong>map:</strong></p>
<pre><code># find the subject of each of the pages of the first log 
input_line -&gt; [ip,  topic] 
</code></pre>
<p><strong>reduce:</strong></p>
<pre><code>
Ip -&gt; [topics] -&gt; [ip, most_popular_topic]
</code></pre>
<p>Reduce receives an ip and a list of all topics as input, it simply calculates which of the topics was encountered most often. Thus, the task is solved using the 1st MapReduce, and the actual Join generally takes place inside the map (therefore, if additional aggregation by key was not needed, the MapOnly job could be dispensed with):</p><img data-max-width="256" data-id="c01e7545-8894-4036-86a2-28bbc2321949" alt="" src="https://cdn.javarush.com/images/article/c01e7545-8894-4036-86a2-28bbc2321949/256.jpeg" style="width: 256px;">
img data-max-width="1024" data-id="c546b694-2382-4fae-8483-aa03dd05b7c0" alt="" src="https://cdn.javarush.com/images/article/c546b694-2382-4fae-8483-aa03dd05b7c0/1024.jpeg" style="width: 1024px;">
<p>The MapReduce paradigm was proposed by Google in 2004 in its article <a href="http://research.google.com/archive/mapreduce.html" target="_blank" rel="nofollow">MapReduce: Simplified Data Processing on Large Clusters</a> . Since the proposed article contained a description of the paradigm, but the implementation was missing, several programmers from Yahoo proposed their implementation as part of the work on the nutch web crawler. You can read more about the history of Hadoop in the article <a href="https://gigaom.com/2013/03/04/the-history-of-hadoop-from-4-nodes-to-the-future-of-data/" target="_blank" rel="nofollow">The history of Hadoop: From 4 nodes to the future of data</a> .</p>
<p>Initially, Hadoop was primarily a tool for storing data and running MapReduce tasks, but now Hadoop is a large stack of technologies related in one way or another to processing big data (not only with MapReduce).</p>
<p>The main (core) components of Hadoop are:</p>
<ul>
 <li><strong><a href="http://hadoop.apache.org/docs/r1.2.1/hdfs_design.html" target="_blank" rel="nofollow">Hadoop Distributed File System (HDFS)</a></strong> is a distributed file system that allows you to store information of almost unlimited size.</li>
 <li><strong><a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="nofollow">Hadoop YARN</a></strong> is a framework for cluster resource management and task management, including the MapReduce framework.</li>
 <li><strong>Hadoop common</strong></li>
</ul>
<p>There are also a large number of projects directly related to Hadoop, but not included in the Hadoop core:</p>
<ul>
 <li><strong><a href="https://hive.apache.org/" target="_blank" rel="nofollow">Hive</a></strong> - a tool for SQL-like queries over big data (turns SQL queries into a series of MapReduce tasks);</li>
 <li><strong><a href="https://pig.apache.org/" target="_blank" rel="nofollow">Pig</a></strong> is a programming language for high-level data analysis. One line of code in this language can turn into a sequence of MapReduce tasks;</li>
 <li><strong><a href="http://hbase.apache.org/" target="_blank" rel="nofollow">Hbase</a></strong> is a columnar database that implements the BigTable paradigm;</li>
 <li><strong><a href="http://cassandra.apache.org/" target="_blank" rel="nofollow">Cassandra</a></strong> is a high-performance distributed key-value database;</li>
 <li><strong><a href="https://zookeeper.apache.org/" target="_blank" rel="nofollow">ZooKeeper</a></strong> is a service for distributed configuration storage and synchronization of configuration changes;</li>
 <li><strong><a href="http://mahout.apache.org/" target="_blank" rel="nofollow">Mahout</a></strong> is a big data machine learning library and engine.</li>
</ul>
<p>Separately, I would like to note the <a href="http://spark.apache.org/" target="_blank" rel="nofollow">Apache Spark</a> project , which is an engine for distributed data processing. Apache Spark typically uses Hadoop components such as HDFS and YARN for its work, while itself has recently become more popular than Hadoop:</p><img data-max-width="1024" data-id="5ddec7de-cdcd-4830-84e0-04be56c9ad55" alt="" src="https://cdn.javarush.com/images/article/5ddec7de-cdcd-4830-84e0-04be56c9ad55/1024.jpeg" style="width: 1024px;">
<p>Some of these components will be covered in separate articles in this series of materials, but for now, let's look at how you can start working with Hadoop and put it into practice.</p>
<h2>4.2 Running MapReduce programs on Hadoop</h2>
<p>Now let's look at how to run a MapReduce task on Hadoop. As a task, we will use the classic <strong>WordCount</strong> example , which was discussed in the previous lesson.</p>
<p><strong>Let me remind you the formulation of the problem:</strong> there is a set of documents. It is necessary for each word occurring in the set of documents to count how many times the word occurs in the set.</p>
<h4>Solution:</h4>
<p>Map splits the document into words and returns a set of pairs (word, 1).</p>
<p>Reduce sums the occurrences of each word:</p>
<div class="table-container">
 <table>
  <tbody>
   <tr>
    <td>
     <pre><code>def map(doc):  
for word in doc.split():  
	yield word, 1 
</code></pre></td>
   </tr>
   <tr>
    <td>
     <pre><code>def reduce(word, values):  
	yield word, sum(values)
</code></pre></td>
   </tr>
  </tbody>
 </table>
</div>
<p>Now the task is to program this solution in the form of code that can be executed on Hadoop and run.</p>
<h2>4.3 Method number 1. Hadoop Streaming</h2>
<p>The easiest way to run a MapReduce program on Hadoop is to use the Hadoop streaming interface. The streaming interface assumes that <strong>map</strong> and <strong>reduce</strong> are implemented as programs that take data from stdin and output to <strong>stdout</strong> .</p>
<p>The program that executes the map function is called mapper. The program that executes <strong>reduce</strong> is called, respectively, <strong>reducer</strong> .</p>
<p>The Streaming interface assumes by default that one incoming line in <strong>a mapper</strong> or <strong>reducer</strong> corresponds to one incoming entry for <strong>map</strong> .</p>
<p>The output of the mapper gets to the input of the reducer in the form of pairs (key, value), while all pairs corresponding to the same key:</p>
<ul>
 <li>Guaranteed to be processed by a single launch of the reducer;</li>
 <li>Will be submitted to the input in a row (that is, if one reducer processes several different keys, the input will be grouped by key).</li>
</ul>
<p>So let's implement mapper and reducer in python:</p>
<pre><code>#mapper.py  
import sys  
  
def do_map(doc):  
for word in doc.split():  
	yield word.lower(), 1  
  
for line in sys.stdin:  
	for key, value in do_map(line):  
    	print(key + "\t" + str(value))  
 
#reducer.py  
import sys  
  
def do_reduce(word, values):  
	return word, sum(values)  
  
prev_key = None  
values = []  
  
for line in sys.stdin:  
	key, value = line.split("\t")  
	if key != prev_key and prev_key is not None:  
    	result_key, result_value = do_reduce(prev_key, values)  
    	print(result_key + "\t" + str(result_value))  
    	values = []  
	prev_key = key  
	values.append(int(value))  
  
if prev_key is not None:  
	result_key, result_value = do_reduce(prev_key, values)  
	print(result_key + "\t" + str(result_value))
</code></pre>
<p>The data that Hadoop will process must be stored on HDFS. Let's upload our articles and put them on HDFS. To do this, use the <strong>hadoop fs</strong> command :</p>
<pre><code>wget https://www.dropbox.com/s/opp5psid1x3jt41/lenta_articles.tar.gz  
tar xzvf lenta_articles.tar.gz  
hadoop fs -put lenta_articles 
</code></pre>
<p>The hadoop fs utility supports a large number of methods for manipulating the file system, many of which are identical to the standard linux utilities.</p>
<p>Now let's start the streaming task:</p>
<pre><code>yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\  
 -input lenta_articles\  
 -output lenta_wordcount\  
 -file mapper.py\  
 -file reducer.py\  
 -mapper "python mapper.py"\  
 -reducer "python reducer.py" 
</code></pre>
<p>The yarn utility is used to run and manage various applications (including map-reduce based) on a cluster. Hadoop-streaming.jar is just one example of such a yarn application.</p>
<p>Next are the launch options:</p>
<ul>
 <li>input - folder with source data on hdfs;</li>
 <li>output - folder on hdfs where you want to put the result;</li>
 <li>file - files that are needed during the operation of the map-reduce task;</li>
 <li>mapper is the console command that will be used for the map stage;</li>
 <li>reduce is the console command that will be used for the reduce stage.</li>
</ul>
<p>After launching, you can see the progress of the task in the console and a URL for viewing more detailed information about the task.</p><img data-max-width="512" data-id="0b59ba71-046d-4419-8028-f2d6818cc431" alt="" src="https://cdn.javarush.com/images/article/0b59ba71-046d-4419-8028-f2d6818cc431/512.jpeg" style="width: 512px;">
<p>In the interface available at this URL, you can find out a more detailed task execution status, view the logs of each mapper and reducer (which is very useful in case of failed tasks).</p><img data-max-width="512" data-id="e3d88205-3283-4d00-809b-1e5f8b7ddcbc" alt="" src="https://cdn.javarush.com/images/article/e3d88205-3283-4d00-809b-1e5f8b7ddcbc/512.jpeg" style="width: 512px;">
<p>The result of the work after successful execution is added to HDFS in the folder that we specified in the output field. You can view its contents using the "hadoop fs -ls lenta_wordcount" command.</p>
<p>The result itself can be obtained as follows:</p>
<pre><code>hadoop fs -text lenta_wordcount/* | sort -n -k2,2 | tail -n5  
с	
41  
что	
43  
на	
82  
и	
111  
в	
194 
</code></pre>
<p>The "hadoop fs -text" command displays the contents of the folder in text form. I sorted the result by the number of occurrences of the words. As expected, the most common words in the language are prepositions.</p>
<h2>4.4 Method number 2: use Java</h2>
<p>Hadoop itself is written in java, and Hadoop's native interface is also java-based. Let's show what a native java application for wordcount looks like:</p>
<pre class=" line-numbers language-java" tabindex="0"><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span><code class=" language-java"><span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">IOException</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">StringTokenizer</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>conf<span class="token punctuation">.</span></span><span class="token class-name">Configuration</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>fs<span class="token punctuation">.</span></span><span class="token class-name">Path</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">IntWritable</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">Text</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span></span><span class="token class-name">Job</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span></span><span class="token class-name">Mapper</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span></span><span class="token class-name">Reducer</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>input<span class="token punctuation">.</span></span><span class="token class-name">FileInputFormat</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>mapreduce<span class="token punctuation">.</span>lib<span class="token punctuation">.</span>output<span class="token punctuation">.</span></span><span class="token class-name">FileOutputFormat</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">WordCount</span> <span class="token punctuation">{</span>

	<span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">TokenizerMapper</span>
        	<span class="token keyword">extends</span> <span class="token class-name">Mapper</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Object</span><span class="token punctuation">,</span> <span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">Text</span><span class="token punctuation">,</span> <span class="token class-name">IntWritable</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">{</span>

    	<span class="token keyword">private</span> <span class="token keyword">final</span> <span class="token keyword">static</span> <span class="token class-name">IntWritable</span> one <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">IntWritable</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    	<span class="token keyword">private</span> <span class="token class-name">Text</span> word <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    	<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">map</span><span class="token punctuation">(</span><span class="token class-name">Object</span> key<span class="token punctuation">,</span> <span class="token class-name">Text</span> value<span class="token punctuation">,</span> <span class="token class-name">Context</span> context
    	<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">IOException</span><span class="token punctuation">,</span> <span class="token class-name">InterruptedException</span> <span class="token punctuation">{</span>
        	<span class="token class-name">StringTokenizer</span> itr <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">StringTokenizer</span><span class="token punctuation">(</span>value<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        	<span class="token keyword">while</span> <span class="token punctuation">(</span>itr<span class="token punctuation">.</span><span class="token function">hasMoreTokens</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
            	word<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>itr<span class="token punctuation">.</span><span class="token function">nextToken</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            	context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span> one<span class="token punctuation">)</span><span class="token punctuation">;</span>
        	<span class="token punctuation">}</span>
    	<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>

	<span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">class</span> <span class="token class-name">IntSumReducer</span>
        	<span class="token keyword">extends</span> <span class="token class-name">Reducer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Text</span><span class="token punctuation">,</span><span class="token class-name">IntWritable</span><span class="token punctuation">,</span><span class="token class-name">Text</span><span class="token punctuation">,</span><span class="token class-name">IntWritable</span><span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>
    	<span class="token keyword">private</span> <span class="token class-name">IntWritable</span> result <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">IntWritable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    	<span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span><span class="token class-name">Text</span> key<span class="token punctuation">,</span> <span class="token class-name">Iterable</span><intwritable> values<span class="token punctuation">,</span>
                       	<span class="token class-name">Context</span> context
    	<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">IOException</span><span class="token punctuation">,</span> <span class="token class-name">InterruptedException</span> <span class="token punctuation">{</span>
        	<span class="token keyword">int</span> sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
        	<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">IntWritable</span> val <span class="token operator">:</span> values<span class="token punctuation">)</span> <span class="token punctuation">{</span>
            	sum <span class="token operator">+=</span> val<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        	<span class="token punctuation">}</span>
        	result<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
        	context<span class="token punctuation">.</span><span class="token function">write</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> result<span class="token punctuation">)</span><span class="token punctuation">;</span>
    	<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>

	<span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">Exception</span> <span class="token punctuation">{</span>
    	<span class="token class-name">Configuration</span> conf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    	<span class="token class-name">Job</span> job <span class="token operator">=</span> <span class="token class-name">Job</span><span class="token punctuation">.</span><span class="token function">getInstance</span><span class="token punctuation">(</span>conf<span class="token punctuation">,</span> <span class="token string">"word count"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    	job<span class="token punctuation">.</span><span class="token function">setJarByClass</span><span class="token punctuation">(</span><span class="token class-name">WordCount</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    	job<span class="token punctuation">.</span><span class="token function">setMapperClass</span><span class="token punctuation">(</span><span class="token class-name">TokenizerMapper</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    	job<span class="token punctuation">.</span><span class="token function">setReducerClass</span><span class="token punctuation">(</span><span class="token class-name">IntSumReducer</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    	job<span class="token punctuation">.</span><span class="token function">setOutputKeyClass</span><span class="token punctuation">(</span><span class="token class-name">Text</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    	job<span class="token punctuation">.</span><span class="token function">setOutputValueClass</span><span class="token punctuation">(</span><span class="token class-name">IntWritable</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    	<span class="token class-name">FileInputFormat</span><span class="token punctuation">.</span><span class="token function">addInputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"hdfs://localhost/user/cloudera/lenta_articles"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    	<span class="token class-name">FileOutputFormat</span><span class="token punctuation">.</span><span class="token function">setOutputPath</span><span class="token punctuation">(</span>job<span class="token punctuation">,</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span><span class="token string">"hdfs://localhost/user/cloudera/lenta_wordcount"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    	<span class="token class-name">System</span><span class="token punctuation">.</span><span class="token function">exit</span><span class="token punctuation">(</span>job<span class="token punctuation">.</span><span class="token function">waitForCompletion</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span> <span class="token operator">?</span> <span class="token number">0</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</intwritable></code></pre>
<p>This class does exactly the same as our Python example. We create the TokenizerMapper and IntSumReducer classes by deriving from the Mapper and Reducer classes, respectively. The classes passed as template parameters specify the types of input and output values. The native API assumes that the map function is given a key-value pair as input. Since in our case the key is empty, we simply define Object as the key type.</p>
<p>In the Main method, we start the mapreduce task and define its parameters - name, mapper and reducer, the path in HDFS, where the input data is located and where to put the result. To compile, we need hadoop libraries. I use Maven to build, for which cloudera has a repository. Instructions for setting it up can be found here. As a result, the pom.xmp file (which is used by maven to describe the assembly of the project) I got the following):</p>
<pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;  
&lt;project xmlns="http://maven.apache.org/POM/4.0.0"  
     	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
     	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;  
	&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;  
  
	&lt;repositories&gt;  
    	&lt;repository&gt;  
        	&lt;id&gt;cloudera&lt;/id&gt;  
        	&lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;  
    	&lt;/repository&gt;  
	&lt;/repositories&gt;  
  
	&lt;dependencies&gt;  
    	&lt;dependency&gt;  
        	&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;  
        	&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;  
        	&lt;version&gt;2.6.0-cdh5.4.2&lt;/version&gt;  
    	&lt;/dependency&gt;  
  
    	&lt;dependency&gt;  
        	&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;  
        	&lt;artifactId&gt;hadoop-auth&lt;/artifactId&gt;  
        	&lt;version&gt;2.6.0-cdh5.4.2&lt;/version&gt;  
    	&lt;/dependency&gt;  
  
    	&lt;dependency&gt;  
        	&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;  
        	&lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;  
        	&lt;version&gt;2.6.0-cdh5.4.2&lt;/version&gt;  
    	&lt;/dependency&gt;  
  
    	&lt;dependency&gt;  
        	&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;  
        	&lt;artifactId&gt;hadoop-mapreduce-client-app&lt;/artifactId&gt;  
        	&lt;version&gt;2.6.0-cdh5.4.2&lt;/version&gt;  
    	&lt;/dependency&gt;  
  
	&lt;/dependencies&gt;  
  
	&lt;groupId&gt;org.dca.examples&lt;/groupId&gt;  
	&lt;artifactId&gt;wordcount&lt;/artifactId&gt;  
	&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  
 
&lt;/project&gt;
</code></pre>
<p>Let's compile the project into a jar package:</p>
<pre><code>mvn clean package
</code></pre>
<p>After building the project into a jar file, the launch occurs in a similar way, as in the case of the streaming interface:</p>
<pre><code>yarn jar wordcount-1.0-SNAPSHOT.jar  WordCount 
</code></pre>
<p>We wait for execution and check the result:</p>
<pre><code>hadoop fs -text lenta_wordcount/* | sort -n -k2,2 | tail -n5  
с	
41  
что	
43  
на	
82  
и	
111  
в	
194 
</code></pre>
<p>As you might guess, the result of running our native application is the same as the result of the streaming application that we launched in the previous way.</p>